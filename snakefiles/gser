"""
genome size estimation rarefaction

This snakefile's main goal is to create a fast, scalable rarefaction analysis
for k-mer based genome size estimation.

This version works with KMC, and not jellyfish
kmc is faster

HOW TO RUN - only one job at a time (absolute minimum of disk space)
   snakemake --cores 90 -j1 -r -p --snakefile gser/snakefiles/gser

HOW TO RUN - (disk space can take up as much as one sequencing run at a time)
   snakemake --cores 90 -r -p --snakefile gser/snakefiles/gser

author: dts
github: @conchoecia
date: dec 2022
license: none atm

Software requirements:
  - fastqsplitter - https://github.com/LUMC/fastqsplitter
    - install with conda: https://anaconda.org/bioconda/fastqsplitter
       conda install fastqsplitter
  - ffmpeg - https://www.tecmint.com/install-ffmpeg-in-linux/
"""

import gzip
from Bio import SeqIO
import os
import pandas as pd
import seaborn as sns

configfile: "config.yaml"

config["tool"] = "gser_analysis"
config["num_splits"] = 100
config["ks"] = config["k-sizes"]

# We need a function to test whether the other tools exist or not.
# This is useful for users who want to run their own binaries
from shutil import which
def is_tool(name):
    """
    From https://stackoverflow.com/a/34177358/5843327 

    Check whether `name` is on PATH and marked as executable.
    """
    return which(name) is not None

# we need this hard-coded directory to access the bin
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))

print(" TESTING FOR REQUIRED SOFTWARE", file = sys.stderr)
# Figure out how to run genomescope and kmc. We need to determine if the user has run make or not.
# start with genomescope
run_progpaths = False
progpaths = {"genomescope": os.path.join(filepath, "../bin/genomescope2.0/genomescope.R"),
             "bioawk"     : os.path.join(filepath, "../bin/bioawk/bioawk")}

num_spaces = max([len(x) for x in progpaths.keys()]) + 1

if not run_progpaths:
    for thisprog in progpaths:
        print(" - {pname: <{ss}} : ".format(pname = thisprog.upper(), ss = num_spaces), file = sys.stderr, end = "")
        # will be either genomescope_path, kmc_path, etc.
        thispath = "{}_path".format(thisprog)
        if thispath not in config:
            config[thispath] = progpaths[thisprog]
            if not os.path.exists(progpaths[thisprog]):
                print("NOT FOUND", file = sys.stderr)
                raise IOError("You have opted to run gser with the program-coupled program, {}, however the path does not exist. Please run 'make' in the gser directory, or specify the path in the config file (see example).".format(thisprog))
        else:
            # The user must have specified the path in the config file.
            # We need to check whether the path exists or not.
            if not is_tool(config[thispath]):
                print("NOT FOUND", file = sys.stderr)
                raise IOError("The program {} specified in the config does not exist or does not execute. Please either specify the path in the config file or comment this line out in the config file (see example).".format(thisprog))
        print("FOUND HERE {}".format(config[thispath]), file = sys.stderr)
    run_progpaths = True

# First we must get the final reads for each sample. This is because the reads can be either a file or a directory.
# For each entry of the reads, check whether it is a directory or a file.
# Make this code safe regardless of whether the filepath ends with '/' or not.
#  Save all of those reads to a new list, and then replace the old list with the new one.
reads_output = []
for sample in config["sample"]:
    reads_for_this_sample = []
    for i in range(len(config["sample"][sample]["reads"])):
        if os.path.isdir(config["sample"][sample]["reads"][i]):
            dirname = config["sample"][sample]["reads"][i]
            if dirname.endswith("/"):
                dirname = dirname[:-1]
            # only keep the files that end with .fastq.gz or .fq.gz
            for fn in os.listdir(dirname):
                if fn.endswith(".fastq.gz") or fn.endswith(".fq.gz"):
                    reads_for_this_sample.append(dirname + "/" + fn)
        else:
            # raise an error if the file doesn't end with .fq.gz or .fastq.gz
            fn = config["sample"][sample]["reads"][i]
            if fn.endswith(".fastq.gz") or fn.endswith(".fq.gz"):
                reads_for_this_sample.append(fn)
            else:
                raise IOError("The file {fn} does not end with .fq.gz or .fastq.gz".format(fn=fn))

    # now that we have all the reads for this sample, replace the old list with the new one 
    config["sample"][sample]["reads"] = reads_for_this_sample

# turn the data structure into something easier to work with later. we can access individual indices
config["reads"] = {}
for sample in config["sample"]:
    config["reads"][sample] = {i:config["sample"][sample]["reads"][i] for i in range(len(config["sample"][sample]["reads"]))}

# check whether each sample has fields "het_or_hom_peak_larger" field
for sample in config["sample"]:
    if "het_or_hom_peak_larger" not in config["sample"][sample]:
        raise ValueError("Each sample must have a field 'het_or_hom_peak_larger'")

# now define which reads we want, since different samples will have different numbers of reads
for sample in config["sample"]:
    for i in range(len(config["sample"][sample]["reads"])):
        for fn in range(0,config["num_splits"]):
            reads_output.append("/fastqs/{sample}/{sample}.{i}.{num}.fastq.gz".format(
                sample=sample, i=i, num=fn ))

wildcard_constraints:
    num= '|'.join([str(i) for i in range(0,config["num_splits"])]),

rule all:
    input:
        expand("/summary_tables/{sample}.{k}.summary_table.tsv",
            sample=config["sample"],
            k = config["ks"]),
        expand("/genomesize_plots/{sample}.{k}.genomesize.pdf",
            sample=config["sample"],
            k = config["ks"]),
        expand(c"/gifs/{sample}.{k}.gif",
            sample=config["sample"],
            k = config["ks"]),
        #pairplot
        expand("/genomesize_plots/{sample}.{k}.pairgrid.pdf",
            sample=config["sample"], k = config["ks"])

rule nkmer:
    input:
        fastq = lambda wildcards: config["reads"][wildcards.sample][int(wildcards.index)]    
    output:
        histo = expand("/histos/{sample}/{sample}_k{k}_part{num}.histo", num=list(range(0,config["num_splits"]))),
        stats = expand("/histos/{sample}/{sample}_k{k}_part{num}.tsv",   num=list(range(0,config["num_splits"])))
    threads: 1
    params:
        k = lambda wildcards: wildcards.k
    shell:
        """
        nkmer --histo-max 10000  -k {params.k} -n {config[num_splits]} -o /histos/{sample}/{sample} {input.fastq}
        """

rule run_genomescope_on_each_histo:
    input:
        histo   = "/histos/{sample}/{sample}_k{k}_part{num}.histo"
    output:
        plot    = "/genomescope/{sample}/{sample}.{k}.{num}_linear_plot.png",
        summary = "/genomescope/{sample}/{sample}.{k}.{num}_summary.txt"
    params:
        outdir = lambda wildcards: "/genomescope/{sample}/".format(
                                    sample=wildcards.sample,
                                    ),
        outprefix = lambda wildcards: "{sample}.{k}.{num}".format(
                                       sample=wildcards.sample,
                                       k=wildcards.k,
                                       num=wildcards.num
                                       ),
        sample = lambda wildcards: wildcards.sample,
        k = lambda wildcards: wildcards.k,
    priority: 5
    threads: 1
    shell:
        """
        COMMAND={config[genomescope_path]}
        if command -v genomescope2 &> /dev/null
        then
            COMMAND=genomescope2
        fi
        ${{COMMAND}} -i {input.histo} -o {params.outdir} \
          -k {params.k} -n {params.outprefix}
        """


# Implementation of algorithm from https://stackoverflow.com/a/22640362/6029703
import numpy as np

def thresholding_algo(y, lag, threshold, influence):
    signals = np.zeros(len(y))
    filteredY = np.array(y)
    avgFilter = [0]*len(y)
    stdFilter = [0]*len(y)
    avgFilter[lag - 1] = np.mean(y[0:lag])
    stdFilter[lag - 1] = np.std(y[0:lag])
    for i in range(lag, len(y)):
        if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter [i-1]:
            if y[i] > avgFilter[i-1]:
                signals[i] = 1
            else:
                signals[i] = -1

            filteredY[i] = influence * y[i] + (1 - influence) * filteredY[i-1]
            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])
            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])
        else:
            signals[i] = 0
            filteredY[i] = y[i]
            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])
            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])

    return dict(signals = np.asarray(signals),
                avgFilter = np.asarray(avgFilter),
                stdFilter = np.asarray(stdFilter))

def remove_repeating_values_from_list(inputlist):
    """
    This function takes a list of ints or strings and makes a reduced representation

    input:  ["a", "a", 0, 1, 1, 1, 1, "b", 1, 1, "a", 5]
    output: ["a", 0, 1, "b", 1, "a", 5]
    """
    repdict = {-1.0: "D", 1.0: "U"}
    inputlist = [repdict[x] for x in inputlist if x in repdict]
    charcounter = []
    new_list    = []
    for i in range(0, len(inputlist)):
        if i == 0:
            new_list.append(inputlist[i])
            charcounter.append(1)
        elif inputlist[i] != inputlist[i - 1]:
            new_list.append(inputlist[i])
            charcounter.append(1)
        else:
            charcounter[-1] += 1
    return ["{}{}".format(charcounter[i], new_list[i]) for i in range(len(new_list))]

rule manual_genome_size_estimation:
    input:
        histo   = "/histos/{sample}/{sample}_k{k}_part{num}.histo"
    output:
        summary = "/manual_estimate/{sample}/{sample}.{k}.{num}_manual_estimate.txt"
    params:
        het_or_hom = lambda wildcards: config["sample"][wildcards.sample]["het_or_hom_peak_larger"]
    priority: 6
    threads: 1
    run:
        outfile = open(output.summary, "w")
        import sys

        spectrum = input.histo
        #this option lets you specify whether the biggest peak is the het or hom peak
        het_or_hom = params.het_or_hom

        # make sure that het or hom is specified
        if het_or_hom not in ["het", "hom"]:
            raise IOError("The third option must be 'het' or 'hom'. This option specifies whether the largest peak is the het or hom peak")

        spec = {}

        # read in the histogram
        maxcov = 10000
        thresh_indices = []
        thresh_values  = []
        with open(spectrum, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    fields = [int(x) for x in line.split()]
                    # field 0 is coverage (x-axis)
                    # field 1 is count (y-axis)
                    if fields[0] <= maxcov:
                        spec[fields[0]] = fields[1]
                        thresh_indices.append(fields[0])
                        thresh_values.append( fields[1])

        lag = 5
        threshold = 2
        influence = 1.0
        thresh_array = thresholding_algo(thresh_values, lag, threshold, influence)
        thresh_dict = dict(zip(thresh_indices, thresh_array["signals"]))

        # Determine whether we see -1.0 or 1.0 first, and the index
        #  1.0 is going up from the valley
        # -1.0 is going down from a peak
        #  If we find -1.0 first, then that means the long trend is flat, and the algorithm found the
        firstU = 9999999
        # get first U
        for i in range(10000):
            if i in thresh_dict:
                if thresh_dict[i] == 1.0:
                    firstU = i
                    break
        # get the last D in the first string of Ds
        # We want:
        #           * <- This D
        #  DDDDDDDDDDUUUUUUDDDDDD
        # This is where the dip is
        seen_D = False
        lastOfFirstDs = 9999999
        for i in range(10000):
            if i in thresh_dict:
                # -1 is D, 1 is U
                if thresh_dict[i] == -1.0:
                    seen_D = True
                    lastOfFirstDs = i
                elif thresh_dict[i] == 1.0:
                    if seen_D:
                        # we have seen a D, now we see a U and we leave
                        break
                    else:
                        # we have not seen a D yet, so we keep going
                        pass


        transition_pattern = remove_repeating_values_from_list(thresh_array["signals"])[:10]

        # first we must find the cutoff. Start with a higher cutoff and work down
        done = False
        absmin = 5
        # this block could be written more simply, but I think this is more readable
        if transition_pattern[0][-1] == "U":
            cutoff = max(firstU - 1, absmin)
        elif transition_pattern[0][-1] == "D":
            Dval = int(transition_pattern[0][:-1])
            Uval = int(transition_pattern[1][:-1])
            DtoV = Dval/Uval
            ratiocutoff = 20
            if DtoV > ratiocutoff:
                cutoff = 15
            else:
                cutoff = max(lastOfFirstDs+1, absmin)
        else:
            raise IOError("There should never be another character here.")

        while not done:
            # measure the spectrum for plotting
            # midmax will be 50 chars wide
            midmax = max([spec[x] for x in range(cutoff, max(spec)) if x in spec])
            maxix = list(spec.keys())[list(spec.values()).index(midmax)]
            if maxix == cutoff:
                cutoff -= 1
            else:
                done = True
            # use lessthan because we select midmax and maxix above
            if cutoff < absmin:
                done = True

        # change the divisor depending on whether the peak was het or hom
        if het_or_hom == "het":
            hetpeak = maxix
            hompeak = maxix*2
        elif het_or_hom == "hom":
            hetpeak = int(maxix/2)
            hompeak = maxix
        else:
            raise IOError("het_or_hom has to be 'het' or 'hom'.")

        ## check if there is a min between cutoff and hetpeak
        # this should default to 6 if not
        print(absmin, cutoff, hetpeak, hompeak)
        # if we're working with a low-coverage dataset, just use the cutoff
        #  to estimate the genome size
        if (hetpeak < absmin):
            cutoff = absmin
        # otherwise try to find min between the cutoff and het peak
        else:
            minsearch = {x:spec[x] for x in range(absmin,hetpeak+1) if x in spec}
            midmin = min([minsearch[x] for x in range(absmin, hetpeak+1)])
            minix = list(minsearch.keys())[list(minsearch.values()).index(midmin)]
            cutoff = minix
        print(absmin, cutoff, hetpeak, hompeak)
        print()

        # these values are used for plotting
        charwidth = 100
        valperchar = int(midmax/charwidth)

        #print out a kmer spectrum
        granularity = 1
        print("# Kmer spectrum visualization", file=outfile)
        print("# X = {} kmers".format(valperchar), file=outfile)
        print("# ------------ is the cutoff for noise kmers", file=outfile)
        print("#  everything at or below this line ---- will be counted for genome size", file=outfile)
        print("#", file=outfile)
        print("# D = down from peak, U = up from valley", file = outfile)
        print("#", file=outfile)
        print("# Transition Pattern: {}".format(" ".join(transition_pattern)), file=outfile)
        print("#", file=outfile)
        print("# {}".format("".join(["="]*(charwidth + 4))), file=outfile)
        cutoffDone = False
        for i in range(1,hompeak*2,granularity):
            #val = int((spec[i] + spec[i-1])/2)
            val = spec[i]
            valwidth = int(val/valperchar)
            numXs = min(valwidth,charwidth)
            print("# {:03} {}".format(i, "".join(["X"]*numXs)), end = "", file=outfile)
            # we print whether this is a peak or valley
            if i in thresh_dict:
                if thresh_dict[i] == -1.0:
                    print(" D", end = "", file=outfile)
                elif thresh_dict[i] == 1.0:
                    print(" U", end = "", file=outfile)
            # print the cutoff
            if i >= cutoff and not cutoffDone:
                print("{}".format("".join(["-"] * (charwidth  - numXs))), end = "", file=outfile)
                cutoffDone = True
            if i == maxix:
                print(" <- estimated {} peak".format(het_or_hom), end = "", file=outfile)
            print("", file=outfile)

        # calculate the genome size
        genome_size = 0
        for i in range(cutoff,max(spec)):
            if i in spec:
                genome_size += spec[i] * i

        genome_size = int(genome_size/hompeak)
        print("cutoff: {}".format(cutoff), file=outfile)
        print("hompeak: {}".format(hompeak), file=outfile)
        print("hetpeak: {}".format(hetpeak), file=outfile)
        print("genomesize: {}".format(genome_size), file=outfile)

rule make_an_animation_for_each_sample:
    """
    just makes an animation for each sample. Shows how the kmer spectrum changes with data amount
    """
    input:
        pngs = expand("/genomescope/{{sample}}/{{sample}}.{{k}}.{num}_linear_plot.png",
            num=list(range(0,config["num_splits"])))
    output:
        gif = "/gifs/{sample}.{k}.gif"
    params:
        pngs = lambda wildcards: ",".join("/genomescope/{sample}/{sample}.{k}.{num}_linear_plot.png".format(
            sample = wildcards.sample, k = wildcards.k, num=i) for i in range(0,config["num_splits"])]),
        prefix = lambda wildcards: "/genomescope/{sample}/{sample}.{k}.".format(
            sample = wildcards.sample, k = wildcards.k)
    priority: 7
    threads: 1
    shell:
        """
        ffmpeg -f image2 -i {params.prefix}%d_linear_plot.png -vf scale=512:-1 {output.gif}
        """

rule make_a_summary_table:
    """
    Make a summary table of the genomescope results for each num and k

    The contents of each input file look like this, and we need to parse it:
    GenomeScope version 2.0
    input file = GGSR_analysis/histos/Hcal/Hcal.21.57.histo
    output directory = GGSR_analysis/genomescope/Hcal/
    p = 2
    k = 21
    name prefix = Hcal.21.57
    
    property                      min               max
    Homozygous (aa)               96.573%           96.9026%
    Heterozygous (ab)             3.0974%           3.42695%
    Genome Haploid Length         100,974,888 bp    103,012,576 bp
    Genome Repeat Length          34,098,896 bp     34,787,017 bp
    Genome Unique Length          66,875,991 bp     68,225,559 bp
    Model Fit                     72.2123%          97.2243%
    Read Error Rate               0.217933%         0.217933%

    For each sample I also need to add the information on each sample.
    sample  fastq_index     mean_readlen    num_reads       gigabases
    Hcal    96              129             2353800         0.305954

    """
    input:
        summaries  = expand("/genomescope/{{sample}}/{{sample}}.{{k}}.{num}_summary.txt",
            num=list(range(0,config["num_splits"]))),
        fastqstats =  expand("/histos/{sample}/{sample}_k{k}_part{num}.tsv",   num=list(range(0,config["num_splits"]))),
        manual = expand(c"/manual_estimate/{{sample}}/{{sample}}.{{k}}.{num}_manual_estimate.txt",
            num=list(range(0,config["num_splits"]))),
    output:
        summary_table = "/summary_tables/{sample}.{k}.summary_table.tsv"
    priority: 8
    threads: 1
    run:
        import pandas as pd
        entries = []

        list_of_results = []
        for thisnum in range(0,config["num_splits"]):
            # set up the dictionary
            dict_of_vals = {}
            dict_of_vals["sample"] = wildcards.sample
            dict_of_vals["fraction_of_total_data"] = (thisnum + 1) / config["num_splits"]
            dict_of_vals["k"] = wildcards.k

            # first we add the info from the fastq files
            targetfile = "/histos/{sample}/{sample}_k{k}_part{num}.tsv".format(
                sample = wildcards.sample, num=thisnum)
            readdf = pd.read_csv(targetfile, sep="\t").iloc[0].to_dict()
            dict_of_vals["num_reads"] = readdf["num_reads"]
            dict_of_vals["gigabases"] = readdf["gigabases"]

            # second, we add the info from 
            targetfile = "/manual_estimate/{sample}/{sample}.{k}.{num}_manual_estimate.txt".format(
                sample = wildcards.sample, k = wildcards.k, num=thisnum)
            with open(targetfile, "r") as f:
                for line in f:
                    line = line.strip()
                    if line and line[0] != "#":
                        splitd = line.split(": ")
                        dict_of_vals["manual_{}".format(splitd[0])] = int(splitd[1])

            # last, we add the results of genomescope
            targetfile = "/genomescope/{sample}/{sample}.{k}.{num}_summary.txt".format(
                sample = wildcards.sample, k = wildcards.k, num=thisnum)
            with open(targetfile, "r") as f:
                start_count = False
                counter = 0
                for line in f:
                    line = line.strip()
                    if line:
                        splitd = line.split()
                        if splitd[0] == "property":
                            start_count = True
                        if start_count:
                            if counter == 1:
                                dict_of_vals["min_hom"] = float(splitd[2].strip("%"))
                                dict_of_vals["max_hom"] = float(splitd[3].strip("%"))
                            elif counter == 2:
                                dict_of_vals["min_het"] = float(splitd[2].strip("%"))
                                dict_of_vals["max_het"] = float(splitd[3].strip("%"))
                            elif counter == 3:
                                dict_of_vals["min_hap_len"] =  splitd[3].replace("," , "")
                                dict_of_vals["max_hap_len"] =  splitd[5].replace("," , "")
                            elif counter == 4:
                                dict_of_vals["min_rep_len"] =  splitd[3].replace("," , "")
                                dict_of_vals["max_rep_len"] =  splitd[5].replace("," , "")
                            elif counter == 5:
                                dict_of_vals["min_uniq_len"] = splitd[3].replace("," , "")
                                dict_of_vals["max_uniq_len"] = splitd[5].replace("," , "")
                            elif counter == 6:
                                dict_of_vals["min_model_fit"] = float(splitd[2].strip("%"))
                                dict_of_vals["max_model_fit"] = float(splitd[3].strip("%"))
                            elif counter == 7:
                                dict_of_vals["min_read_error_rate"] = float(splitd[3].strip("%"))
                                dict_of_vals["max_read_error_rate"] = float(splitd[4].strip("%"))
                            counter += 1
            
            # lastly, append everything to the list of results
            list_of_results.append(dict_of_vals)
        df = pd.DataFrame(list_of_results)
        # get the cumsum of the num_reads and gigabases
        df["num_reads"] = df["num_reads"].cumsum()
        df["gigabases"] = df["gigabases"].cumsum()
        print(df)
        df.to_csv(output.summary_table, sep="\t", index=False)

rule make_genome_estimate_plot:
    """
    Make a python plot with the genome size estimates (min and max). Should be colored between the min and max
    """
    input:
        summary_table  = "/summary_tables/{sample}.{k}.summary_table.tsv"
    output:
        genomesize_pdf = "/genomesize_plots/{sample}.{k}.genomesize.pdf"
    priority: 9
    threads: 1
    run:
        # import the necessary libraries to make a plot with matplotlib
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        #read the summary table into pandas
        df = pd.read_csv(input.summary_table, sep="\t")

        #make a plot
        # the x-axis is the fraction of the total data
        x = df["fraction_of_total_data"]
        y1 = df["min_hap_len"]/1000000
        y2 = df["max_hap_len"]/1000000
        y3 = y1 + ((y2 - y1)/2)

        plt.plot(x, y1, lw=0)
        plt.plot(x, y2, lw = 0)
        
        # Fill the area between the two lines
        plt.fill_between(x, y1, y2)
        plt.plot(x, y3, lw = 0.5, color = "black")

        # plot the manual estimate
        manual_y = df["manual_genomesize"]/1000000
        plt.plot(x, manual_y, lw = 0.5, color = "blue")


        # set the y-axis label as "Mb"
        plt.ylabel("Mb")

        # set the x-axis label as "fraction of total data"
        plt.xlabel("fraction of total data")

        # set the title as the sample and k
        plt.title("{sample}, k-{k} genome size estimate".format(sample = wildcards.sample, k = wildcards.k))
        
        # Save the plot as a pdf file
        plt.savefig(output.genomesize_pdf)

rule make_pairgrid_plot:
    """
    Make a grid of the variables
    """
    input:
        summary_table  = "/summary_tables/{sample}.{k}.summary_table.tsv"
    output:
        genomesize_pdf = "/genomesize_plots/{sample}.{k}.pairgrid.pdf"
    threads: 1
    run:
        # import the necessary libraries to make a plot with matplotlib
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        import seaborn as sns
        #read the summary table into pandas
        df = pd.read_csv(input.summary_table, sep="\t")
        df["manual_genomesize"] = df["manual_genomesize"]/1000000
        df["min_hap_len"] = df["min_hap_len"]/1000000
        df["min_uniq_len"] = df["min_uniq_len"]/1000000

        # make a pairgrid plot
        g = sns.PairGrid(df, vars=["fraction_of_total_data", "num_reads", "gigabases",
                                   "manual_cutoff", "manual_hompeak", 
                                   "manual_genomesize", "min_het", "min_hap_len", "min_uniq_len"])
        g.map_upper(plt.scatter)
        g.map_lower(plt.scatter)
        g.savefig(output.genomesize_pdf)
