"""
genome size estimation rarefaction

This snakefile's main goal is to create a fast, scalable rarefaction analysis
for k-mer based genome size estimation.

This version works with KMC, and not jellyfish
kmc is faster

HOW TO RUN
snakemake --cores 90 -j1 -r -p --snakefile ../gser/snakefiles/gser

the -j1 is important, because it prevents snakemake from running multiple jobs at once.

Works by breaking the fastq files into 100 pieces,
author: dts
github: @conchoecia
date: dec 2022
license: none atm

Software requirements:
  - fastqsplitter - https://github.com/LUMC/fastqsplitter
    - install with conda: https://anaconda.org/bioconda/fastqsplitter
       conda install fastqsplitter
  - bioawk - https://github.com/lh3/bioawk
    - install with conda: https://anaconda.org/bioconda/bioawk
       conda install -c bioconda bioawk
  - ffmpeg - https://www.tecmint.com/install-ffmpeg-in-linux/
  -

On the test dataset, this is the disk usage:
    78G    ./jellyfish
    87M    ./genomescope
    20K    ./genomesize_plots
    5.2M   ./gifs
    7.4M   ./histos
    19G    ./fastqs
    20K    ./summary_tables
    96G    .

With the recursive implementation, the disk usage is:
    792M   ./jellyfish
    20K    ./histos
    19G    ./fastqs
    20G    .

-----------------------
with jellyfish and one thread the kmer counting of one dataset was:
real    3m45.128s
user    3m57.922s
sys     0m15.067s

with jellyfish and 90 threads the kmer counting was:
real    0m24.158s
user    6m46.728s
sys     0m52.862s

with  the same dataset, one thread, and kmc3 the counting took
real    2m54.955s
user    2m56.587s
sys     0m9.309s

with kmc3 and 90 threads the program took:
real    0m11.938s
user    4m6.031s
sys     0m11.111s

-----------------------
341% speedup using kmc instead of jellyfish
kmc version on Hcal dataset takes this long with kmc
real    70m50.780s
user    440m21.775s
sys     52m2.915s

and this long with jellyfish
real    235m30.655s
user    653m21.973s
sys     72m28.589s
"""

import gzip
from Bio import SeqIO
import os
import pandas as pd
import seaborn as sns

configfile: "config.yaml"

config["tool"] = "gser_analysis"
config["num_splits"] = 100
config["ks"] = config["k-sizes"]

# we need this hard-coded directory to access the bin
filepath = os.path.dirname(os.path.realpath(workflow.snakefile))

# turn the data structure into something easier to work with later. we can access individual indices
config["reads"] = {}
for sample in config["sample"]:
    config["reads"][sample] = {i:config["sample"][sample]["reads"][i] for i in range(len(config["sample"][sample]["reads"]))}

# check whether each sample has fields "het_or_hom_peak_larger" field
for sample in config["sample"]:
    if "het_or_hom_peak_larger" not in config["sample"][sample]:
        raise ValueError("Each sample must have a field 'het_or_hom_peak_larger'")

# now define which reads we want, since different samples will have different numbers of reads
reads_output = []
for sample in config["sample"]:
    for i in range(len(config["sample"][sample]["reads"])):
        for fn in range(0,config["num_splits"]):
            reads_output.append(config["tool"] + "/fastqs/{sample}/{sample}.{i}.{num}.fastq.gz".format(
                sample=sample, i=i, num=fn ))

wildcard_constraints:
    num= '|'.join([str(i) for i in range(0,config["num_splits"])]),

rule all:
    input:
        expand(config["tool"] + "/summary_tables/{sample}.{k}.summary_table.tsv",
            sample=config["sample"],
            k = config["ks"]),
        expand(config["tool"] + "/genomesize_plots/{sample}.{k}.genomesize.pdf",
            sample=config["sample"],
            k = config["ks"]),
        expand(config["tool"] + "/gifs/{sample}.{k}.gif",
            sample=config["sample"],
            k = config["ks"]),
        #pairplot
        expand(config["tool"] + "/genomesize_plots/{sample}.{k}.pairgrid.pdf",
            sample=config["sample"], k = config["ks"])

rule randomize_and_split_one_fastq_per_sample:
    """
    This rule splits the files into 100 pieces each.
     This was 120 times faster than splitting them with python.

    This software can be installed with fastqsplitter:
      https://github.com/LUMC/fastqsplitter

    There is an easy way to install this with:
      conda install fastqsplitter
    """
    input:
        fastq = lambda wildcards: config["reads"][wildcards.sample][int(wildcards.index)]
    output:
        fastqs = temp(expand(config["tool"] + "/fastqs/{{sample}}/{{sample}}.{{index}}.{num}.fastq.gz",
                        num=list(range(0,config["num_splits"]))))
    params:
        outstring = lambda wildcards: " -o ".join([ config["tool"] + "/fastqs/{sample}/{sample}.{index}.{num}.fastq.gz".format(
                        sample = wildcards.sample, index = wildcards.index, num=i) for i in range(0,config["num_splits"])])
    priority: 1
    threads: 1
    shell:
        """
        # make a system call to fastqsplitter to split the file into 100 pieces
        fastqsplitter -i {input.fastq} -o {params.outstring}
        """

rule collect_info_about_fastqs:
    """
    This prints stats about individual fastq files to a file for later parsing.
    This will be used to make plots later.

    We will collect:
      - Mean length of reads
      - Number of reads
      - Number of gigabases
    """
    input:
        fastqs = lambda wildcards: expand(config["tool"] + "/fastqs/{{sample}}/{{sample}}.{index}.{{num}}.fastq.gz",
                        index=range(0,len(config["sample"][wildcards.sample]["reads"])))
    output:
        stats = config["tool"] + "/fastqs/{sample}/{sample}.{num}.read_stats.tsv"
    params:
        sample = lambda wildcards: wildcards.sample,
        num = lambda wildcards: wildcards.num
    priority: 2
    threads: 1
    shell:
        """
        zcat {input.fastqs} | \
        bioawk -c fastx 'BEGIN{{bases = 0}} \
                         {{bases = bases + length($seq)}} \
                         END{{printf("sample\\tfastq_index\\tmean_readlen\\tnum_reads\\tgigabases\\n%s\\t%d\\t%d\\t%d\\t%f\\n", \
                              "{params.sample}", {params.num}, int(bases/NR), NR, bases/1000000000)}}' > {output.stats}
        """

rule list_of_fastq_files_by_num:
    """
    kmc uses a list of fastq files as input.
    """
    input:
        fastqs = lambda wildcards: expand(config["tool"] + "/fastqs/{{sample}}/{{sample}}.{index}.{{num}}.fastq.gz",
                        index=range(0,len(config["sample"][wildcards.sample]["reads"])))
    output:
        fastq_list = config["tool"] + "/fastqs/{sample}/{sample}.{num}.fastq_list.txt"
    threads: 1
    run:
        with open(output.fastq_list, "w") as f:
            for fastq in input.fastqs:
                print(os.path.abspath(fastq), file = f)

rule run_kmc_on_the_zeroth_fastq:
    """
    - Runs kmc on the 0th fastq chunk for every sample.
    """
    input:
        fastq_list = config["tool"] + "/fastqs/{sample}/{sample}.0.fastq_list.txt",
        fastqs = lambda wildcards: expand(config["tool"] + "/fastqs/{{sample}}/{{sample}}.{index}.0.fastq.gz",
                        index=range(0,len(config["sample"][wildcards.sample]["reads"]))),
        kmc_path       = os.path.join(filepath, "../bin/kmc3/bin/kmc"),
        kmc_tools_path = os.path.join(filepath, "../bin/kmc3/bin/kmc_tools")
    output:
        kmcdb  = temp(config["tool"] + "/kmc/{sample}/{sample}.{k}.0.kmc_pre"),
        kmcsuf = temp(config["tool"] + "/kmc/{sample}/{sample}.{k}.0.kmc_suf"),
        tempdir     = temp(directory("{sample}_0_{k}_dir")), 
        histo       = config["tool"] + "/histos/{sample}/{sample}.{k}.0.histo"
    params:
        outprefix = config["tool"] + "/kmc/{sample}/{sample}.{k}.0",
        k = lambda wildcards: wildcards.k,
        num = 0,
        sample = lambda wildcards: wildcards.sample
    priority: 3
    threads: workflow.cores - 1
    shell:
        """
        mkdir {output.tempdir}
        {input.kmc_path} -ci1 -cs10000 -t{threads} -k{params.k} @{input.fastq_list} {params.outprefix} {output.tempdir}/
        {input.kmc_tools_path} transform {params.outprefix} histogram {output.histo} -ci1 -cx10000
        """

rule run_kmc_on_every_other_fastq:
    """
    - Runs kmc on each of the 100 fastq chunks for every sample.
    - Because there can be many fastq files, it works by concatenating the pieces from each
        original fastq file into kmc.
    - The output file for this will be a database instead of a histo file.
    """
    input:
        prev_kmcpre = lambda wildcards: config["tool"] + "/kmc/{sample}/{sample}.{k}.{num}.kmc_pre".format(
                                    sample = wildcards.sample,
                                    num = int(wildcards.num)-1,
                                    k = wildcards.k),
        prev_kmcsuf = lambda wildcards: config["tool"] + "/kmc/{sample}/{sample}.{k}.{num}.kmc_suf".format(
                                    sample = wildcards.sample,
                                    num = int(wildcards.num)-1,
                                    k = wildcards.k),
        fastq_list = config["tool"] + "/fastqs/{sample}/{sample}.{num}.fastq_list.txt",
        fastqs     = lambda wildcards: [config["tool"] + "/fastqs/{sample}/{sample}.{index}.{num}.fastq.gz".format(
                                    sample = wildcards.sample,
                                    index = i,
                                    num = wildcards.num) for i in range(0,len(config["sample"][wildcards.sample]["reads"]))],
        kmc_path       = os.path.join(filepath, "../bin/kmc3/bin/kmc"),
        kmc_tools_path = os.path.join(filepath, "../bin/kmc3/bin/kmc_tools")
    output:
        kmcdb  = temp(config["tool"] + "/kmc/{sample}/{sample}.{k}.{num}.kmc_pre"),
        kmcsuf = temp(config["tool"] + "/kmc/{sample}/{sample}.{k}.{num}.kmc_suf"),
        tempdir     = temp(directory("{sample}_{num}_{k}_dir")), 
        histo       = config["tool"] + "/histos/{sample}/{sample}.{k}.{num}.histo"
    params:
        prev_prefix = lambda wildcards: config["tool"] + "/kmc/{sample}/{sample}.{k}.{num}".format(
                                    sample = wildcards.sample,
                                    num = int(wildcards.num)-1,
                                    k = wildcards.k),   
        outprefix = config["tool"] + "/kmc/{sample}/{sample}.{k}.{num}",
        k = lambda wildcards: wildcards.k,
        num = 0,
        sample = lambda wildcards: wildcards.sample
    priority: 4
    threads: workflow.cores - 1
    shell:
        """
        # generate the KMC counts
        mkdir {output.tempdir}
        {input.kmc_path} -ci1 -cs10000 -t{threads} -k{params.k} @{input.fastq_list} {params.outprefix}TEMP {output.tempdir}/

        # merge the kmc files
        {input.kmc_tools_path} simple {params.prev_prefix} -ci1 -cx10000 {params.outprefix}TEMP -ci1 -cx10000 union {params.outprefix} -ocsum -ci1 -cs10000
        rm {params.outprefix}TEMP*

        # generate the histogram
        {input.kmc_tools_path} transform {params.outprefix} histogram {output.histo} -ci1 -cx10000
        """

rule run_genomescope_on_each_histo:
    input:
        histo   = config["tool"] + "/histos/{sample}/{sample}.{k}.{num}.histo",
        genomescope_path       = os.path.join(filepath, "../bin/genomescope2.0/genomescope.R"),
    output:
        plot    = config["tool"] + "/genomescope/{sample}/{sample}.{k}.{num}_linear_plot.png",
        summary = config["tool"] + "/genomescope/{sample}/{sample}.{k}.{num}_summary.txt"
    params:
        outdir = lambda wildcards: "{tool}/genomescope/{sample}/".format(
                                    tool=config["tool"],
                                    sample=wildcards.sample,
                                    ),
        outprefix = lambda wildcards: "{sample}.{k}.{num}".format(
                                       sample=wildcards.sample,
                                       k=wildcards.k,
                                       num=wildcards.num
                                       ),
        sample = lambda wildcards: wildcards.sample,
        k = lambda wildcards: wildcards.k,
    priority: 5
    threads: 1
    shell:
        """
        COMMAND={input.genomescope_path}
        if command -v genomescope2 &> /dev/null
        then
            COMMAND=genomescope2
        fi
        ${{COMMAND}} -i {input.histo} -o {params.outdir} \
          -k {params.k} -n {params.outprefix}
        """


# Implementation of algorithm from https://stackoverflow.com/a/22640362/6029703
import numpy as np

def thresholding_algo(y, lag, threshold, influence):
    signals = np.zeros(len(y))
    filteredY = np.array(y)
    avgFilter = [0]*len(y)
    stdFilter = [0]*len(y)
    avgFilter[lag - 1] = np.mean(y[0:lag])
    stdFilter[lag - 1] = np.std(y[0:lag])
    for i in range(lag, len(y)):
        if abs(y[i] - avgFilter[i-1]) > threshold * stdFilter [i-1]:
            if y[i] > avgFilter[i-1]:
                signals[i] = 1
            else:
                signals[i] = -1

            filteredY[i] = influence * y[i] + (1 - influence) * filteredY[i-1]
            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])
            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])
        else:
            signals[i] = 0
            filteredY[i] = y[i]
            avgFilter[i] = np.mean(filteredY[(i-lag+1):i+1])
            stdFilter[i] = np.std(filteredY[(i-lag+1):i+1])

    return dict(signals = np.asarray(signals),
                avgFilter = np.asarray(avgFilter),
                stdFilter = np.asarray(stdFilter))

def remove_repeating_values_from_list(inputlist):
    """
    This function takes a list of ints or strings and makes a reduced representation

    input:  ["a", "a", 0, 1, 1, 1, 1, "b", 1, 1, "a", 5]
    output: ["a", 0, 1, "b", 1, "a", 5]
    """
    repdict = {-1.0: "D", 1.0: "U"}
    inputlist = [repdict[x] for x in inputlist if x in repdict]
    charcounter = []
    new_list    = []
    for i in range(0, len(inputlist)):
        if i == 0:
            new_list.append(inputlist[i])
            charcounter.append(1)
        elif inputlist[i] != inputlist[i - 1]:
            new_list.append(inputlist[i])
            charcounter.append(1)
        else:
            charcounter[-1] += 1
    return ["{}{}".format(charcounter[i], new_list[i]) for i in range(len(new_list))]

rule manual_genome_size_estimation:
    input:
        histo   = config["tool"] + "/histos/{sample}/{sample}.{k}.{num}.histo"
    output:
        summary = config["tool"] + "/manual_estimate/{sample}/{sample}.{k}.{num}_manual_estimate.txt"
    params:
        het_or_hom = lambda wildcards: config["sample"][wildcards.sample]["het_or_hom_peak_larger"]
    priority: 6
    threads: 1
    run:
        outfile = open(output.summary, "w")
        import sys

        spectrum = input.histo
        #this option lets you specify whether the biggest peak is the het or hom peak
        het_or_hom = params.het_or_hom

        # make sure that het or hom is specified
        if het_or_hom not in ["het", "hom"]:
            raise IOError("The third option must be 'het' or 'hom'. This option specifies whether the largest peak is the het or hom peak")

        spec = {}

        # read in the histogram
        maxcov = 10000
        thresh_indices = []
        thresh_values  = []
        with open(spectrum, "r") as f:
            for line in f:
                line = line.strip()
                if line:
                    fields = [int(x) for x in line.split()]
                    # field 0 is coverage (x-axis)
                    # field 1 is count (y-axis)
                    if fields[0] <= maxcov:
                        spec[fields[0]] = fields[1]
                        thresh_indices.append(fields[0])
                        thresh_values.append( fields[1])

        lag = 5
        threshold = 2
        influence = 1.0
        thresh_array = thresholding_algo(thresh_values, lag, threshold, influence)
        thresh_dict = dict(zip(thresh_indices, thresh_array["signals"]))

        # Determine whether we see -1.0 or 1.0 first, and the index
        #  1.0 is going up from the valley
        # -1.0 is going down from a peak
        #  If we find -1.0 first, then that means the long trend is flat, and the algorithm found the
        firstU = 9999999
        # get first U
        for i in range(10000):
            if i in thresh_dict:
                if thresh_dict[i] == 1.0:
                    firstU = i
                    break
        # get the last D in the first string of Ds
        # We want:
        #           * <- This D
        #  DDDDDDDDDDUUUUUUDDDDDD
        # This is where the dip is
        seen_D = False
        lastOfFirstDs = 9999999
        for i in range(10000):
            if i in thresh_dict:
                # -1 is D, 1 is U
                if thresh_dict[i] == -1.0:
                    seen_D = True
                    lastOfFirstDs = i
                elif thresh_dict[i] == 1.0:
                    if seen_D:
                        # we have seen a D, now we see a U and we leave
                        break
                    else:
                        # we have not seen a D yet, so we keep going
                        pass


        transition_pattern = remove_repeating_values_from_list(thresh_array["signals"])[:10]

        # first we must find the cutoff. Start with a higher cutoff and work down
        done = False
        absmin = 5
        # this block could be written more simply, but I think this is more readable
        if transition_pattern[0][-1] == "U":
            cutoff = max(firstU - 1, absmin)
        elif transition_pattern[0][-1] == "D":
            Dval = int(transition_pattern[0][:-1])
            Uval = int(transition_pattern[1][:-1])
            DtoV = Dval/Uval
            ratiocutoff = 20
            if DtoV > ratiocutoff:
                cutoff = 15
            else:
                cutoff = max(lastOfFirstDs+1, absmin)
        else:
            raise IOError("There should never be another character here.")

        while not done:
            # measure the spectrum for plotting
            # midmax will be 50 chars wide
            midmax = max([spec[x] for x in range(cutoff, max(spec)) if x in spec])
            maxix = list(spec.keys())[list(spec.values()).index(midmax)]
            if maxix == cutoff:
                cutoff -= 1
            else:
                done = True
            # use lessthan because we select midmax and maxix above
            if cutoff < absmin:
                done = True

        # change the divisor depending on whether the peak was het or hom
        if het_or_hom == "het":
            hetpeak = maxix
            hompeak = maxix*2
        elif het_or_hom == "hom":
            hetpeak = int(maxix/2)
            hompeak = maxix
        else:
            raise IOError("het_or_hom has to be 'het' or 'hom'.")

        ## check if there is a min between cutoff and hetpeak
        # this should default to 6 if not
        print(absmin, cutoff, hetpeak, hompeak)
        # if we're working with a low-coverage dataset, just use the cutoff
        #  to estimate the genome size
        if (hetpeak < absmin):
            cutoff = absmin
        # otherwise try to find min between the cutoff and het peak
        else:
            minsearch = {x:spec[x] for x in range(absmin,hetpeak+1) if x in spec}
            midmin = min([minsearch[x] for x in range(absmin, hetpeak+1)])
            minix = list(minsearch.keys())[list(minsearch.values()).index(midmin)]
            cutoff = minix
        print(absmin, cutoff, hetpeak, hompeak)
        print()

        # these values are used for plotting
        charwidth = 100
        valperchar = int(midmax/charwidth)

        #print out a kmer spectrum
        granularity = 1
        print("# Kmer spectrum visualization", file=outfile)
        print("# X = {} kmers".format(valperchar), file=outfile)
        print("# ------------ is the cutoff for noise kmers", file=outfile)
        print("#  everything at or below this line ---- will be counted for genome size", file=outfile)
        print("#", file=outfile)
        print("# D = down from peak, U = up from valley", file = outfile)
        print("#", file=outfile)
        print("# Transition Pattern: {}".format(" ".join(transition_pattern)), file=outfile)
        print("#", file=outfile)
        print("# {}".format("".join(["="]*(charwidth + 4))), file=outfile)
        cutoffDone = False
        for i in range(1,hompeak*2,granularity):
            #val = int((spec[i] + spec[i-1])/2)
            val = spec[i]
            valwidth = int(val/valperchar)
            numXs = min(valwidth,charwidth)
            print("# {:03} {}".format(i, "".join(["X"]*numXs)), end = "", file=outfile)
            # we print whether this is a peak or valley
            if i in thresh_dict:
                if thresh_dict[i] == -1.0:
                    print(" D", end = "", file=outfile)
                elif thresh_dict[i] == 1.0:
                    print(" U", end = "", file=outfile)
            # print the cutoff
            if i >= cutoff and not cutoffDone:
                print("{}".format("".join(["-"] * (charwidth  - numXs))), end = "", file=outfile)
                cutoffDone = True
            if i == maxix:
                print(" <- estimated {} peak".format(het_or_hom), end = "", file=outfile)
            print("", file=outfile)

        # calculate the genome size
        genome_size = 0
        for i in range(cutoff,max(spec)):
            if i in spec:
                genome_size += spec[i] * i

        genome_size = int(genome_size/hompeak)
        print("cutoff: {}".format(cutoff), file=outfile)
        print("hompeak: {}".format(hompeak), file=outfile)
        print("hetpeak: {}".format(hetpeak), file=outfile)
        print("genomesize: {}".format(genome_size), file=outfile)

rule make_an_animation_for_each_sample:
    """
    just makes an animation for each sample. Shows how the kmer spectrum changes with data amount
    """
    input:
        pngs = expand(config["tool"] + "/genomescope/{{sample}}/{{sample}}.{{k}}.{num}_linear_plot.png",
            num=list(range(0,config["num_splits"])))
    output:
        gif = config["tool"] + "/gifs/{sample}.{k}.gif"
    params:
        pngs = lambda wildcards: ",".join([config["tool"] + "/genomescope/{sample}/{sample}.{k}.{num}_linear_plot.png".format(
            sample = wildcards.sample, k = wildcards.k, num=i) for i in range(0,config["num_splits"])]),
        prefix = lambda wildcards: config["tool"] + "/genomescope/{sample}/{sample}.{k}.".format(
            sample = wildcards.sample, k = wildcards.k)
    priority: 7
    threads: 1
    shell:
        """
        ffmpeg -f image2 -i {params.prefix}%d_linear_plot.png -vf scale=512:-1 {output.gif}
        """

rule make_a_summary_table:
    """
    Make a summary table of the genomescope results for each num and k

    The contents of each input file look like this, and we need to parse it:
    GenomeScope version 2.0
    input file = GGSR_analysis/histos/Hcal/Hcal.21.57.histo
    output directory = GGSR_analysis/genomescope/Hcal/
    p = 2
    k = 21
    name prefix = Hcal.21.57
    
    property                      min               max
    Homozygous (aa)               96.573%           96.9026%
    Heterozygous (ab)             3.0974%           3.42695%
    Genome Haploid Length         100,974,888 bp    103,012,576 bp
    Genome Repeat Length          34,098,896 bp     34,787,017 bp
    Genome Unique Length          66,875,991 bp     68,225,559 bp
    Model Fit                     72.2123%          97.2243%
    Read Error Rate               0.217933%         0.217933%

    For each sample I also need to add the information on each sample.
    sample  fastq_index     mean_readlen    num_reads       gigabases
    Hcal    96              129             2353800         0.305954

    """
    input:
        summaries  = expand(config["tool"] + "/genomescope/{{sample}}/{{sample}}.{{k}}.{num}_summary.txt",
            num=list(range(0,config["num_splits"]))),
        fastqstats = expand(config["tool"] + "/fastqs/{{sample}}/{{sample}}.{num}.read_stats.tsv",
            num=list(range(0,config["num_splits"]))),
        manual = expand(config["tool"] + "/manual_estimate/{{sample}}/{{sample}}.{{k}}.{num}_manual_estimate.txt",
            num=list(range(0,config["num_splits"]))),
    output:
        summary_table = config["tool"] + "/summary_tables/{sample}.{k}.summary_table.tsv"
    priority: 8
    threads: 1
    run:
        import pandas as pd
        entries = []

        list_of_results = []
        for thisnum in range(0,config["num_splits"]):
            # set up the dictionary
            dict_of_vals = {}
            dict_of_vals["sample"] = wildcards.sample
            dict_of_vals["fraction_of_total_data"] = (thisnum + 1) / config["num_splits"]
            dict_of_vals["k"] = wildcards.k

            # first we add the info from the fastq files
            targetfile = config["tool"] + "/fastqs/{sample}/{sample}.{num}.read_stats.tsv".format(
                sample = wildcards.sample, num=thisnum)
            readdf = pd.read_csv(targetfile, sep="\t").iloc[0].to_dict()
            dict_of_vals["num_reads"] = readdf["num_reads"]
            dict_of_vals["gigabases"] = readdf["gigabases"]

            # second, we add the info from 
            targetfile = config["tool"] + "/manual_estimate/{sample}/{sample}.{k}.{num}_manual_estimate.txt".format(
                sample = wildcards.sample, k = wildcards.k, num=thisnum)
            with open(targetfile, "r") as f:
                for line in f:
                    line = line.strip()
                    if line and line[0] != "#":
                        splitd = line.split(": ")
                        dict_of_vals["manual_{}".format(splitd[0])] = int(splitd[1])

            # last, we add the results of genomescope
            targetfile = config["tool"] + "/genomescope/{sample}/{sample}.{k}.{num}_summary.txt".format(
                sample = wildcards.sample, k = wildcards.k, num=thisnum)
            with open(targetfile, "r") as f:
                start_count = False
                counter = 0
                for line in f:
                    line = line.strip()
                    if line:
                        splitd = line.split()
                        if splitd[0] == "property":
                            start_count = True
                        if start_count:
                            if counter == 1:
                                dict_of_vals["min_hom"] = float(splitd[2].strip("%"))
                                dict_of_vals["max_hom"] = float(splitd[3].strip("%"))
                            elif counter == 2:
                                dict_of_vals["min_het"] = float(splitd[2].strip("%"))
                                dict_of_vals["max_het"] = float(splitd[3].strip("%"))
                            elif counter == 3:
                                dict_of_vals["min_hap_len"] =  splitd[3].replace("," , "")
                                dict_of_vals["max_hap_len"] =  splitd[5].replace("," , "")
                            elif counter == 4:
                                dict_of_vals["min_rep_len"] =  splitd[3].replace("," , "")
                                dict_of_vals["max_rep_len"] =  splitd[5].replace("," , "")
                            elif counter == 5:
                                dict_of_vals["min_uniq_len"] = splitd[3].replace("," , "")
                                dict_of_vals["max_uniq_len"] = splitd[5].replace("," , "")
                            elif counter == 6:
                                dict_of_vals["min_model_fit"] = float(splitd[2].strip("%"))
                                dict_of_vals["max_model_fit"] = float(splitd[3].strip("%"))
                            elif counter == 7:
                                dict_of_vals["min_read_error_rate"] = float(splitd[3].strip("%"))
                                dict_of_vals["max_read_error_rate"] = float(splitd[4].strip("%"))
                            counter += 1
            
            # lastly, append everything to the list of results
            list_of_results.append(dict_of_vals)
        df = pd.DataFrame(list_of_results)
        # get the cumsum of the num_reads and gigabases
        df["num_reads"] = df["num_reads"].cumsum()
        df["gigabases"] = df["gigabases"].cumsum()
        print(df)
        df.to_csv(output.summary_table, sep="\t", index=False)

rule make_genome_estimate_plot:
    """
    Make a python plot with the genome size estimates (min and max). Should be colored between the min and max
    """
    input:
        summary_table  = config["tool"] + "/summary_tables/{sample}.{k}.summary_table.tsv"
    output:
        genomesize_pdf = config["tool"] + "/genomesize_plots/{sample}.{k}.genomesize.pdf"
    priority: 9
    threads: 1
    run:
        # import the necessary libraries to make a plot with matplotlib
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        #read the summary table into pandas
        df = pd.read_csv(input.summary_table, sep="\t")

        #make a plot
        # the x-axis is the fraction of the total data
        x = df["fraction_of_total_data"]
        y1 = df["min_hap_len"]/1000000
        y2 = df["max_hap_len"]/1000000
        y3 = y1 + ((y2 - y1)/2)

        plt.plot(x, y1, lw=0)
        plt.plot(x, y2, lw = 0)
        
        # Fill the area between the two lines
        plt.fill_between(x, y1, y2)
        plt.plot(x, y3, lw = 0.5, color = "black")

        # plot the manual estimate
        manual_y = df["manual_genomesize"]/1000000
        plt.plot(x, manual_y, lw = 0.5, color = "blue")


        # set the y-axis label as "Mb"
        plt.ylabel("Mb")

        # set the x-axis label as "fraction of total data"
        plt.xlabel("fraction of total data")

        # set the title as the sample and k
        plt.title("{sample}, k-{k} genome size estimate".format(sample = wildcards.sample, k = wildcards.k))
        
        # Save the plot as a pdf file
        plt.savefig(output.genomesize_pdf)

rule make_pairgrid_plot:
    """
    Make a grid of the variables
    """
    input:
        summary_table  = config["tool"] + "/summary_tables/{sample}.{k}.summary_table.tsv"
    output:
        genomesize_pdf = config["tool"] + "/genomesize_plots/{sample}.{k}.pairgrid.pdf"
    threads: 1
    run:
        # import the necessary libraries to make a plot with matplotlib
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        import seaborn as sns
        #read the summary table into pandas
        df = pd.read_csv(input.summary_table, sep="\t")
        df["manual_genomesize"] = df["manual_genomesize"]/1000000
        df["min_hap_len"] = df["min_hap_len"]/1000000
        df["min_uniq_len"] = df["min_uniq_len"]/1000000

        # make a pairgrid plot
        g = sns.PairGrid(df, vars=["fraction_of_total_data", "num_reads", "gigabases",
                                   "manual_cutoff", "manual_hompeak", 
                                   "manual_genomesize", "min_het", "min_hap_len", "min_uniq_len"])
        g.map_upper(plt.scatter)
        g.map_lower(plt.scatter)
        g.savefig(output.genomesize_pdf)
